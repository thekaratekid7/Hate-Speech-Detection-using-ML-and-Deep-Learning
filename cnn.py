# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1guelbMiydTqnrYFjIB85vI0dV_JRREOu
"""

import pandas as pd
import numpy as np
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
import keras
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, Flatten, CuDNNLSTM, Bidirectional
from keras import layers
#from keras.layers.embeddings import Embedding
from keras.initializers import Constant

#from keras_self_attention import SeqSelfAttention, SeqWeightedAttention

from sklearn.model_selection import train_test_split
import json

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import RandomizedSearchCV

import time

start_time = time.time()

#fetching preprocessed data upto stemming of all the description
#print("fetching combined stemmed words\n")
#pre_text = p.combinedStemmedWords()
dataset=pd.read_csv('/content/drive/My Drive/Hate Speech Detection/Project Codes/naive inp.csv')
#print(dataset)

def removeSpaces(dataset):
  ret = []
  for data in dataset:
    ret.append(data.strip())

  return ret

x = dataset.drop('class',axis=1)
x = list(x["tweet"])
X = removeSpaces(x)
y = list(dataset['class'])

#finding maxlen parameter for padding
max_length = max([len(s) for s in X])

#creating an embedding dictionary
print("creating embedding_index\n")
embeddings_index = {}
'''
f = open("all_in_one_20/cbow_model.txt")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:])
    embeddings_index[word] = coefs
    
f.close()
'''
#vetorize the text samples
print("vetorizing the text descriptions\n")
tokenizer_obj = Tokenizer()
tokenizer_obj.fit_on_texts(X)
sequences = tokenizer_obj.texts_to_sequences(X)

#padding sequences
word_index = tokenizer_obj.word_index
padded_pre_text = pad_sequences(sequences, maxlen = max_length, padding = 'post', truncating = 'post')

#######NO NEED TO SHUFFLE AGAIN SINCE ITS ALREADY SHUFFLED######
'''
#shuffling features and labels (ie shuffling both in same order)
indices = np.arange(padded_pre_text.shape[0])
np.random.shuffle(indices)
padded_pre_text = padded_pre_text[indices]

#importing labels
labels = pd.read_csv(r"labels.csv")
#making the label as of type array
labels = labels["data"].values
#shuffling labels in the same order as features
labels = labels[indices]
'''
#######NO NEED TO SHUFFLE AGAIN SINCE ITS ALREADY SHUFFLED######

#importing labels
labels = y

#one-hot encoding labels
one_hot_labels = pd.get_dummies(labels)
#converting to numpy array
one_hot_labels = np.array(one_hot_labels)


#splitting train and test into 70 and 30 respectively
X_train, X_test, y_train, y_test = train_test_split(padded_pre_text, one_hot_labels, test_size=0.30, random_state=4)

#creating embedding layer for lstm
print("creating embedding layer for lstm\n")
num_words = len(word_index) + 1
EMBEDDING_DIM = 100
embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))

for word,i in word_index.items():
    if i > num_words:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        #words not found in embedding index will be all-zeros
        embedding_matrix[i] = embedding_vector

model_count=0
#defining model
print("model definition\n")
def create_model(optimizer,dense,num_filters,kernel_size):

    #count for the number of models build
    global model_count
    model_count+=1

    #making these global, otherwise wont be able to access inside the method
    global embedding_matrix
    global max_length
    global EMBEDDING_DIM
    global num_words
    
    print("\nCONFIGS USED "+"FOR MODEL NO: ",model_count)
    print("===============")
    print("optimizer - ",optimizer)
    print("dense - ",dense)
    '''
    print("dropout - ",dropout)
    print("learning_rate - ",learning_rate)
    
    
    if optimizer == 'RMSprop':
        optimizer = keras.optimizers.RMSprop(lr=learning_rate)
    elif optimizer == 'Adam':
        optimizer = keras.optimizers.Adam(lr=learning_rate)
    '''

    model = Sequential()
    embedding_layer = Embedding(num_words,EMBEDDING_DIM,
                                embeddings_initializer=Constant(embedding_matrix),
                                input_length=max_length,
                                trainable=False,
                                name="c_c_p")
    
    model.add(embedding_layer)

    #model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))
    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu')) 
    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu')) 
    model.add(layers.MaxPooling1D()) 

    model.add(Flatten())

    if dense == 3:
      model.add(Dense(100, activation='relu'))
      model.add(Dense(80, activation='relu'))
      model.add(Dense(40, activation='relu'))

    if dense == 2:
      model.add(Dense(100, activation='relu'))
      model.add(Dense(80, activation='relu'))

    if dense == 1:
      model.add(Dense(100, activation='relu'))

    model.add(Dense(13, activation='softmax'))
    
    #compiling model
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    
    print(model.summary())
    print("\n")
    
    return model

model = KerasClassifier(build_fn=create_model, verbose=1)

params = dict(batch_size = [100,200,300,400,500,600,700,800,900,1000], 
                  epochs = [10,15,20],
                  optimizer = ['RMSprop','Adam'],
                  dense = [1,2,3],
                  num_filters=[32, 64, 128],
                  kernel_size=[3, 5, 7]
                  #conv_layers = [2,3],
                  #learning_rate = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],
                  #momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9],
                  #dropout = [0.0, 0.1, 0.2, 0.3, 0.4]
              )


#grid = GridSearchCV(estimator=model, param_grid = params, cv=3)
random = RandomizedSearchCV(estimator = model, param_distributions = params, cv = 5, return_train_score = True, verbose=1)
#n_jobs=-1

random_result = random.fit(X_train, y_train, validation_split = 0.2)

stop_time = time.time()

total_time = (stop_time - start_time)

print("\n============================================\n")
print("--- %s seconds ---" % total_time)
print("\n============================================\n")